{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXocNyH76T8o",
        "outputId": "ef7567ed-c0d5-4a39-aa1b-a5e87636c17e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp38-cp38-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1 MB 1.2 MB/s eta 0:15:44tcmalloc: large alloc 1147494400 bytes == 0x39b1c000 @  0x7f63ddcd9615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 1.2 MB/s eta 0:12:25tcmalloc: large alloc 1434370048 bytes == 0x7e172000 @  0x7f63ddcd9615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 1.2 MB/s eta 0:08:55tcmalloc: large alloc 1792966656 bytes == 0x2fa4000 @  0x7f63ddcd9615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.2 MB/s eta 0:04:04tcmalloc: large alloc 2241208320 bytes == 0x6dd8c000 @  0x7f63ddcd9615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1982242816 bytes == 0xf36ee000 @  0x7f63ddcd81e7 0x4d30a0 0x4d312c 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91\n",
            "tcmalloc: large alloc 2477809664 bytes == 0x1dde06000 @  0x7f63ddcd9615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91 0x5d8941 0x4fe318\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 5.6 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (4.4.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.8.0+cu111 which is incompatible.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.8.0+cu111 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu111\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVt4wblF6cW1",
        "outputId": "8e45fa83-90c6-489a-c26a-64ded75f187d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (4.64.1)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (1.8.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0->torchtext==0.9.0) (4.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.14.0\n",
            "    Uninstalling torchtext-0.14.0:\n",
            "      Successfully uninstalled torchtext-0.14.0\n",
            "Successfully installed torchtext-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz4yTrZN8gOf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import random\n",
        "\n",
        "SEED = 1039\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=input_dim,\n",
        "            embedding_dim=emb_dim\n",
        "        )\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hid_dim,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "\n",
        "        embedded = self.embedding(src)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=output_dim,\n",
        "            embedding_dim=emb_dim\n",
        "        )\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hid_dim,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.out = nn.Linear(\n",
        "            in_features=hid_dim,\n",
        "            out_features=output_dim\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "\n",
        "        prediction = self.out(output.squeeze(0))\n",
        "\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            input = (trg[t] if teacher_force else top1)\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6S-5m7rLx8d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "\n",
        "from torchtext.legacy.data import Field, Dataset\n",
        "from torchtext.legacy.datasets import TranslationDataset\n",
        "from torchtext.legacy import data\n",
        "\n",
        "\n",
        "class PunctuationDataset(Dataset):\n",
        "    urls = []  # insert our link from google drive\n",
        "    name = ''\n",
        "    dirname = 'punc'\n",
        "\n",
        "    @staticmethod\n",
        "    def sort_key(ex):\n",
        "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
        "\n",
        "    def __init__(self, path, exts, fields, **kwargs):\n",
        "        if not isinstance(fields[0], (tuple, list)):\n",
        "            fields = [('src', fields[0]), ('trg', fields[1])]\n",
        "\n",
        "        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
        "\n",
        "        examples = []\n",
        "        with io.open(src_path, mode='r', encoding='utf-8') as src_file, \\\n",
        "                io.open(trg_path, mode='r', encoding='utf-8') as trg_file:\n",
        "            for src_line, trg_line in zip(src_file, trg_file):\n",
        "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
        "                if src_line != '' and trg_line != '':\n",
        "                    examples.append(data.Example.fromlist(\n",
        "                        [src_line, trg_line], fields))\n",
        "\n",
        "        super(PunctuationDataset, self).__init__(examples, fields, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, exts, fields, path=None, root='data',\n",
        "               train='train', validation='val', test='test', **kwargs):\n",
        "        \"\"\"Create dataset objects for splits of a TranslationDataset.\n",
        "        Args:\n",
        "            exts: A tuple containing the extension to path for each language.\n",
        "            fields: A tuple containing the fields that will be used for data\n",
        "                in each language.\n",
        "            path (str): Common prefix of the splits' file paths, or None to use\n",
        "                the result of cls.download(root).\n",
        "            root: Root dataset storage directory. Default is '.data'.\n",
        "            train: The prefix of the train data. Default: 'train'.\n",
        "            validation: The prefix of the validation data. Default: 'val'.\n",
        "            test: The prefix of the test data. Default: 'test'.\n",
        "            Remaining keyword arguments: Passed to the splits method of\n",
        "                Dataset.\n",
        "        \"\"\"\n",
        "        if path is None:\n",
        "            path = cls.download(root)\n",
        "\n",
        "        train_data = None if train is None else cls(\n",
        "            os.path.join(path, train), exts, fields, **kwargs)\n",
        "        val_data = None if validation is None else cls(\n",
        "            os.path.join(path, validation), exts, fields, **kwargs)\n",
        "        test_data = None if test is None else cls(\n",
        "            os.path.join(path, test), exts, fields, **kwargs)\n",
        "        return tuple(d for d in (train_data, val_data, test_data)\n",
        "                     if d is not None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A05T7TIG6dSa",
        "outputId": "1f7746f4-44b0-4ff2-855a-cb5c67beaeb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 349143\n",
            "Number of validation examples: 116381\n",
            "Number of testing examples: 116382\n",
            "Unique tokens in source (unmarked) vocabulary: 14113\n",
            "Unique tokens in target (marked) vocabulary: 14136\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator, Dataset\n",
        "\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "SRC = Field(tokenize=tokenize,\n",
        "            init_token='<sos>',\n",
        "            eos_token='<eos>',\n",
        "            lower=True)\n",
        "\n",
        "TRG = Field(tokenize=tokenize,\n",
        "            init_token='<sos>',\n",
        "            eos_token='<eos>',\n",
        "            lower=True)\n",
        "\n",
        "train_data, valid_data, test_data = PunctuationDataset.splits(fields=(SRC, TRG),\n",
        "                                                    exts=('.um', '.m'))\n",
        "\n",
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
        "\n",
        "SRC.build_vocab(train_data, min_freq=10)\n",
        "TRG.build_vocab(train_data, min_freq=10)\n",
        "\n",
        "print(f\"Unique tokens in source (unmarked) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (marked) vocabulary: {len(TRG.vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBY-5Bm96lvB",
        "outputId": "a6260ba7-3f4c-430a-b308-5597cac7c0aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "The model has 21,839,928 trainable parameters\n",
            "['что', 'с', 'вами', 'вчера', 'было']\n",
            "<class 'list'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-539ed502137a>:30: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(param, -0.05, 0.05)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device.type)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform(param, -0.05, 0.05)\n",
        "\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "example_idx = np.random.choice(np.arange(len(test_data)))\n",
        "print(vars(train_data.examples[example_idx])['src'])\n",
        "print(type(vars(train_data.examples[example_idx])['src']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2bqb5iS6v0r"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None, plot_local=False):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    history = []\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        history.append(loss.cpu().data.numpy())\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK_yxAKY6zhm"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    history = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0)\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnaKCKqR62qH"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    tokens = sentence\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for t in range(1, max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "    return trg_tokens[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJmpOHW8-oNz"
      },
      "outputs": [],
      "source": [
        "def punct(sentence):\n",
        "    i = sentence\n",
        "    temp_data = []\n",
        "    for j in range(0, len(i), 2):\n",
        "        if i[j] == 'DASH'.lower():\n",
        "            temp_data.append('-')\n",
        "            continue\n",
        "        if i[j] == 'DIGIT_COMMA'.lower():\n",
        "            temp_data.append(i[j + 1] + ',')\n",
        "            continue\n",
        "        if i[j] == 'DIGIT_O'.lower():\n",
        "            temp_data.append(i[j + 1])\n",
        "            continue\n",
        "        if i[j] == 'DIGIT_PERIOD'.lower():\n",
        "            temp_data.append(i[j + 1] + '.')\n",
        "            continue\n",
        "        if i[j] == 'DIGIT_QUESTION'.lower():\n",
        "            temp_data.append(i[j + 1] + '?')\n",
        "            continue\n",
        "        if i[j] == 'DIGIT_EXCLAMATION'.lower():\n",
        "            temp_data.append(i[j + 1] + '!')\n",
        "            continue\n",
        "        if i[j] == 'DIGIT_COLON'.lower():\n",
        "            temp_data.append(i[j + 1] + ':')\n",
        "            continue\n",
        "        if i[j] == 'DIGIT_COLON_COMMA'.lower():\n",
        "            temp_data.append(i[j + 1] + ';')\n",
        "            continue\n",
        "        if i[j] == 'DIGIT_DASH'.lower():\n",
        "            temp_data.append(i[j + 1] + '-')\n",
        "            continue\n",
        "        if i[j] == 'DIGIT_UNK'.lower():\n",
        "            temp_data.append(i[j + 1])\n",
        "            continue\n",
        "\n",
        "        if i[j] == 'UPPER_O'.lower():\n",
        "            temp_data.append(i[j + 1].capitalize())\n",
        "            continue\n",
        "        if i[j] == 'UPPER_COMMA'.lower():\n",
        "            temp_data.append(i[j + 1].capitalize() + ',')\n",
        "            continue\n",
        "        if i[j] == 'UPPER_PERIOD'.lower():\n",
        "            temp_data.append(i[j + 1].capitalize() + '.')\n",
        "            continue\n",
        "        if i[j] == 'UPPER_QUESTION'.lower():\n",
        "            temp_data.append(i[j + 1].capitalize() + '?')\n",
        "            continue\n",
        "        if i[j] == 'UPPER_EXCLAMATION'.lower():\n",
        "            temp_data.append(i[j + 1].capitalize() + '!')\n",
        "            continue\n",
        "        if i[j] == 'UPPER_COLON'.lower():\n",
        "            temp_data.append(i[j + 1].capitalize() + ':')\n",
        "            continue\n",
        "        if i[j] == 'UPPER_COLON_COMMA'.lower():\n",
        "            temp_data.append(i[j + 1].capitalize() + ';')\n",
        "            continue\n",
        "        if i[j] == 'UPPER_DASH'.lower():\n",
        "            temp_data.append(i[j + 1].capitalize() + '-')\n",
        "            continue\n",
        "        if i[j] == 'UPPER_UNK'.lower():\n",
        "            temp_data.append(i[j + 1].capitalize())\n",
        "            continue\n",
        "\n",
        "        if i[j] == 'LOWER_O'.lower():\n",
        "            temp_data.append(i[j + 1])\n",
        "            continue\n",
        "        if i[j] == 'LOWER_COMMA'.lower():\n",
        "            temp_data.append(i[j + 1] + ',')\n",
        "            continue\n",
        "        if i[j] == 'LOWER_PERIOD'.lower():\n",
        "            temp_data.append(i[j + 1] + '.')\n",
        "            continue\n",
        "        if i[j] == 'LOWER_QUESTION'.lower():\n",
        "            temp_data.append(i[j + 1] + '?')\n",
        "            continue\n",
        "        if i[j] == 'яЦ'.lower():\n",
        "            temp_data.append(i[j + 1] + '!')\n",
        "            continue\n",
        "        if i[j] == 'LOWER_COLON'.lower():\n",
        "            temp_data.append(i[j + 1] + ':')\n",
        "            continue\n",
        "        if i[j] == 'LOWER_COLON_COMMA'.lower():\n",
        "            temp_data.append(i[j + 1] + ';')\n",
        "            continue\n",
        "        if i[j] == 'LOWER_DASH'.lower():\n",
        "            temp_data.append(i[j + 1] + '-')\n",
        "            continue\n",
        "        if i[j] == 'LOWER_UNK'.lower():\n",
        "            temp_data.append(i[j + 1])\n",
        "            continue\n",
        "\n",
        "        if i[j] == 'UNK'.lower():\n",
        "            temp_data.append(i[j + 1])\n",
        "            continue\n",
        "    return \" \".join(temp_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MeKyeAe63gt"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "def get_example_translation():\n",
        "    example_idx = np.random.choice(np.arange(len(valid_data)))\n",
        "    \n",
        "    src = vars(valid_data.examples[example_idx])['src']\n",
        "    trg = vars(valid_data.examples[example_idx])['trg']\n",
        "\n",
        "    src_string = f'src = {\" \".join(src)}'\n",
        "    trg_string = f'trg = {punct(trg)}'\n",
        "\n",
        "    translation = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "    for i in range(1, len(trg), 2):\n",
        "      translation[i] = trg[i]\n",
        "\n",
        "    translation_string = f'predicted trg = {punct(translation)}'\n",
        "    return ('\\n\\n'.join([src_string, trg_string, translation_string]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpUS9vaTW2sO"
      },
      "outputs": [],
      "source": [
        "def find_metric():\n",
        "  accuracy = 0\n",
        "  for i in range(len(valid_data)):\n",
        "    src = vars(valid_data[i])['src']\n",
        "    trg = vars(valid_data[i])['trg']\n",
        "    trans = translate_sentence(src, SRC, TRG, model, device)[:-1]\n",
        "    arr = [1 if a == b else 0 for a, b in zip(trans, trg)]\n",
        "    aux = np.sum(arr) / len(arr)\n",
        "    accuracy += aux\n",
        "  return accuracy / len(valid_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mmYvlMa166g5",
        "outputId": "7a696004-9b3a-4d9c-c9e5-dc3838e3e24e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------\n",
            "src = может быть лучше было не ворошить прошлое\n",
            "\n",
            "trg = Может быть, лучше было не ворошить прошлое.\n",
            "\n",
            "predicted trg = Может быть, лучше было не ворошить прошлое.\n",
            "-----------------------------------------------------------------------------\n",
            "src = кто-то вырвал из книги страницу\n",
            "\n",
            "trg = Кто-то вырвал из книги страницу.\n",
            "\n",
            "predicted trg = Кто-то вырвал из книги страницу.\n",
            "-----------------------------------------------------------------------------\n",
            "src = территорию страны на девяносто процентов составляет пустыня\n",
            "\n",
            "trg = Территорию страны на девяносто процентов составляет пустыня.\n",
            "\n",
            "predicted trg = Территорию страны на девяносто процентов составляет пустыня.\n",
            "-----------------------------------------------------------------------------\n",
            "src = она говорит я приехала сюда вчера\n",
            "\n",
            "trg = Она говорит: Я приехала сюда вчера.\n",
            "\n",
            "predicted trg = Она говорит я приехала сюда вчера.\n",
            "-----------------------------------------------------------------------------\n",
            "src = тебе не хватало уверенности в себе\n",
            "\n",
            "trg = Тебе не хватало уверенности в себе.\n",
            "\n",
            "predicted trg = Тебе не хватало уверенности в себе.\n",
            "-----------------------------------------------------------------------------\n",
            "src = том не разрешит нам это сделать\n",
            "\n",
            "trg = Том не разрешит нам это сделать.\n",
            "\n",
            "predicted trg = Том не разрешит нам это сделать.\n",
            "-----------------------------------------------------------------------------\n",
            "src = хотите устриц или предпочитаете мидии\n",
            "\n",
            "trg = Хотите устриц или предпочитаете мидии?\n",
            "\n",
            "predicted trg = Хотите устриц или предпочитаете мидии.\n",
            "-----------------------------------------------------------------------------\n",
            "src = я хочу чтобы вы делали это правильно\n",
            "\n",
            "trg = Я хочу, чтобы вы делали это правильно.\n",
            "\n",
            "predicted trg = Я хочу, чтобы вы делали это правильно.\n",
            "-----------------------------------------------------------------------------\n",
            "src = том бы никогда такого не сделал\n",
            "\n",
            "trg = Том бы никогда такого не сделал.\n",
            "\n",
            "predicted trg = Том бы никогда такого не сделал.\n",
            "-----------------------------------------------------------------------------\n",
            "src = читать код сложнее чем его писать\n",
            "\n",
            "trg = Читать код сложнее, чем его писать.\n",
            "\n",
            "predicted trg = Читать код сложнее, чем его писать.\n",
            "-----------------------------------------------------------------------------\n",
            "Epoch: 01 | Time: 27m 0s\n",
            "\tTrain Loss: 0.815 | Train PPL:   2.259\n",
            "\t Val. Loss: 0.585 |  Val. PPL:   1.796\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5eacab7f3978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\t Accuracy: {math.exp(find_metric()):7.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-287206186c32>\u001b[0m in \u001b[0;36mfind_metric\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"
          ]
        }
      ],
      "source": [
        "train_history = []\n",
        "valid_history = []\n",
        "\n",
        "N_EPOCHS = 1\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, train_history, valid_history)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # train_history.append(train_loss)\n",
        "    # valid_history.append(valid_loss)\n",
        "\n",
        "    val_example_data = next(iter(valid_iterator))\n",
        "    to_print = []\n",
        "    print('-----------------------------------------------------------------------------')\n",
        "    for i in range(10):\n",
        "        print(get_example_translation())\n",
        "        print('-----------------------------------------------------------------------------')\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Cz4uQXUmO-g",
        "outputId": "4543d1cc-244a-40a5-b661-d917b3324b63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Accuracy: 0.8327190114684226\n"
          ]
        }
      ],
      "source": [
        "print(f'\\t Accuracy: {find_metric()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWbpMP4EAO5h",
        "outputId": "d51c7b20-7fa7-4ca7-a16d-16dcf805ee6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "src = чем ждать тома лучше поедим сейчас пока еда не остыла\n",
            "\n",
            "trg = Чем ждать Тома, лучше поедим сейчас, пока еда не остыла.\n",
            "\n",
            "predicted trg = Чем ждать Тома лучше поедим сейчас, пока еда не остыла.\n",
            "-----------------------------------------------------------\n",
            "src = никто не имеет права указывать мне\n",
            "\n",
            "trg = Никто не имеет права указывать мне!\n",
            "\n",
            "predicted trg = Никто не имеет права указывать мне.\n",
            "-----------------------------------------------------------\n",
            "src = у меня не было достаточно наличных денег поэтому я расплатился кредитной картой\n",
            "\n",
            "trg = У меня не было достаточно наличных денег, поэтому я расплатился кредитной картой.\n",
            "\n",
            "predicted trg = У меня не было достаточно наличных, денег поэтому, я расплатился кредитной картой.\n",
            "-----------------------------------------------------------\n",
            "src = мой номер у вас есть позвоните мне на днях\n",
            "\n",
            "trg = Мой номер у вас есть. Позвоните мне на днях.\n",
            "\n",
            "predicted trg = Мой номер у вас есть позвоните мне на днях.\n",
            "-----------------------------------------------------------\n",
            "src = что ты кистью водишь как по пизде ладошкой\n",
            "\n",
            "trg = Что ты кистью водишь, как по пизде ладошкой?\n",
            "\n",
            "predicted trg = Что ты кистью водишь, как по пизде ладошкой?\n",
            "-----------------------------------------------------------\n",
            "src = задайте этот вопрос себе самому\n",
            "\n",
            "trg = Задайте этот вопрос себе самому.\n",
            "\n",
            "predicted trg = Задайте этот вопрос себе самому.\n",
            "-----------------------------------------------------------\n",
            "src = если тебе понадобится какая-нибудь помощь просто позвони мне\n",
            "\n",
            "trg = Если тебе понадобится какая-нибудь помощь, просто позвони мне!\n",
            "\n",
            "predicted trg = Если тебе понадобится какая-нибудь, помощь просто позвони мне.\n",
            "-----------------------------------------------------------\n",
            "src = я думал что у меня ещё будет возможность это сделать\n",
            "\n",
            "trg = Я думал, что у меня ещё будет возможность это сделать.\n",
            "\n",
            "predicted trg = Я думал, что у меня ещё будет возможность это сделать.\n",
            "-----------------------------------------------------------\n",
            "src = что тебя во мне заинтересовало\n",
            "\n",
            "trg = Что тебя во мне заинтересовало?\n",
            "\n",
            "predicted trg = Что тебя во мне заинтересовало?\n",
            "-----------------------------------------------------------\n",
            "src = они были очень рады получить наше приглашение\n",
            "\n",
            "trg = Они были очень рады получить наше приглашение.\n",
            "\n",
            "predicted trg = Они были очень рады получить наше приглашение.\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print(get_example_translation())\n",
        "  print('-----------------------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}